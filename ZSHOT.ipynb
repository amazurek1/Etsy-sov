{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_46\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_features (InputLayer)     (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sketch_features (InputLayer)    (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 8192)         0           image_features[0][0]             \n",
      "                                                                 sketch_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 4096)         33558528    concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4096)         16384       dense_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 4096)         0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 2048)         8390656     dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 2048)         8192        dense_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 1024)         2098176     batch_normalization_32[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 44,071,936\n",
      "Trainable params: 44,059,648\n",
      "Non-trainable params: 12,288\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_47\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_z (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sketch_features (InputLayer)    (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 5120)         0           input_z[0][0]                    \n",
      "                                                                 sketch_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (None, 2048)         10487808    concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_out (Dense)             (None, 4096)         8392704     dense_95[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 18,880,512\n",
      "Trainable params: 18,880,512\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cpark/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:346: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5/5 [==============================] - 5s 1s/step - loss: 357.9951 - decoder_out_loss: 349.5620 - recons_output_loss: 0.8433\n",
      "Epoch 2/25\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 342.7702 - decoder_out_loss: 335.4778 - recons_output_loss: 0.7292\n",
      "Epoch 3/25\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 323.0690 - decoder_out_loss: 317.1558 - recons_output_loss: 0.5913\n",
      "Epoch 4/25\n",
      "5/5 [==============================] - 1s 213ms/step - loss: 301.9133 - decoder_out_loss: 297.0932 - recons_output_loss: 0.4820\n",
      "Epoch 5/25\n",
      "5/5 [==============================] - 1s 209ms/step - loss: 278.5267 - decoder_out_loss: 274.5880 - recons_output_loss: 0.3939\n",
      "Epoch 6/25\n",
      "5/5 [==============================] - 1s 213ms/step - loss: 257.3237 - decoder_out_loss: 253.8451 - recons_output_loss: 0.3479\n",
      "Epoch 7/25\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 228.9120 - decoder_out_loss: 225.7167 - recons_output_loss: 0.3195\n",
      "Epoch 8/25\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 208.2305 - decoder_out_loss: 205.2337 - recons_output_loss: 0.2997\n",
      "Epoch 9/25\n",
      "5/5 [==============================] - 1s 214ms/step - loss: 188.3676 - decoder_out_loss: 185.6076 - recons_output_loss: 0.2760\n",
      "Epoch 10/25\n",
      "5/5 [==============================] - 1s 214ms/step - loss: 162.6625 - decoder_out_loss: 160.0611 - recons_output_loss: 0.2601\n",
      "Epoch 11/25\n",
      "5/5 [==============================] - 1s 216ms/step - loss: 142.3586 - decoder_out_loss: 139.9273 - recons_output_loss: 0.2431\n",
      "Epoch 12/25\n",
      "5/5 [==============================] - 1s 216ms/step - loss: 126.2702 - decoder_out_loss: 123.9538 - recons_output_loss: 0.2316\n",
      "Epoch 13/25\n",
      "5/5 [==============================] - 1s 216ms/step - loss: 117.9946 - decoder_out_loss: 115.7004 - recons_output_loss: 0.2294\n",
      "Epoch 14/25\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 103.1064 - decoder_out_loss: 100.8339 - recons_output_loss: 0.2273\n",
      "Epoch 15/25\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 92.3196 - decoder_out_loss: 89.8140 - recons_output_loss: 0.2506\n",
      "Epoch 16/25\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 85.1543 - decoder_out_loss: 83.1734 - recons_output_loss: 0.1981\n",
      "Epoch 17/25\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 79.3135 - decoder_out_loss: 77.5667 - recons_output_loss: 0.1747\n",
      "Epoch 18/25\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 73.6892 - decoder_out_loss: 72.0163 - recons_output_loss: 0.1673\n",
      "Epoch 19/25\n",
      "5/5 [==============================] - 1s 216ms/step - loss: 68.9055 - decoder_out_loss: 67.1855 - recons_output_loss: 0.1720\n",
      "Epoch 20/25\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 61.2854 - decoder_out_loss: 59.4311 - recons_output_loss: 0.1854\n",
      "Epoch 21/25\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 56.1220 - decoder_out_loss: 54.5609 - recons_output_loss: 0.1561\n",
      "Epoch 22/25\n",
      "5/5 [==============================] - 1s 213ms/step - loss: 52.3079 - decoder_out_loss: 50.7997 - recons_output_loss: 0.1508\n",
      "Epoch 23/25\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 52.7693 - decoder_out_loss: 51.3468 - recons_output_loss: 0.1422\n",
      "Epoch 24/25\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 47.5605 - decoder_out_loss: 46.1415 - recons_output_loss: 0.1419\n",
      "Epoch 25/25\n",
      "5/5 [==============================] - 1s 213ms/step - loss: 45.5154 - decoder_out_loss: 44.1441 - recons_output_loss: 0.1371\n",
      "Predicting...\n",
      "500/500 [==============================] - 2s 3ms/step\n",
      "\n",
      "The mean precision@200 for test sketches is 0.0\n",
      "The mAP for test_sketches is 0.0\n"
     ]
    }
   ],
   "source": [
    "# trainCVAE_pre\n",
    "from keras.layers import Input, Dense, Lambda, Dropout, Flatten, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import Adam, SGD\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import cv2\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "# ================== LAB RESOURCES ARE LIMITED=================== #\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "def get_session(gpu_fraction=0.5):\n",
    "    '''Assume that you have 6GB of GPU memory and want to allocate ~2GB'''\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "    \n",
    "tf.keras.backend.set_session(\n",
    "    get_session()\n",
    ")\n",
    "# KTF.set_session(get_session())\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "\n",
    "#============================Define constants=======================================#\n",
    "# Some Constants\n",
    "\n",
    "# Batch size and maximum number of epochs\n",
    "MAX_EPOCH = 25\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Input Dimension i.e. image pretrained VGG-net features\n",
    "n_x = 4096\n",
    "\n",
    "# Conditional Variable size i.e. sketch extracted features\n",
    "n_y = 4096\n",
    "\n",
    "# Z size : random variable\n",
    "n_z = 1024\n",
    "internalSize = 2048\n",
    "# path = '../../Datasets/SUN/'\n",
    "\n",
    "sketch_features = Input(shape=[n_y], name='sketch_features')\n",
    "image_features = Input(shape=[n_x] , name='image_features')\n",
    "input_combined = concatenate([image_features, sketch_features])\n",
    "\n",
    "#Construct Encoder\n",
    "temp_h_q = Dense(internalSize*2, activation='relu')(input_combined)\n",
    "temp_h_q_bn = BatchNormalization()(temp_h_q)\n",
    "h_q_zd = Dropout(rate=0.3)(temp_h_q_bn)\n",
    "h_q = Dense(internalSize, activation='relu')(h_q_zd)\n",
    "h_q_bn = BatchNormalization()(h_q)\n",
    "\n",
    "#parameters of hidden variable\n",
    "mu = Dense(n_z, activation='tanh')(h_q_bn)\n",
    "log_sigma = Dense(n_z, activation='tanh')(h_q_bn)\n",
    "\n",
    "#Sampling layer - defined\n",
    "def sample_z(args):\n",
    "    mu, log_sigma = args\n",
    "    eps = K.random_normal(shape=[n_z], mean=0., stddev=1.)\n",
    "    return mu + K.exp(log_sigma / 2) * eps\n",
    "\n",
    "#concatenate sampled z and conditional input i.e. sketch\n",
    "z = Lambda(sample_z)([mu, log_sigma])\n",
    "z_cond = concatenate([z, sketch_features])\n",
    "\n",
    "#Define layers\n",
    "decoder_hidden = Dense(internalSize, activation='relu')\n",
    "decoder_out = Dense(n_x, activation='relu', name='decoder_out')\n",
    "\n",
    "#construct Decoder\n",
    "h_p = decoder_hidden(z_cond)\n",
    "reconstr = decoder_out(h_p)\n",
    "\n",
    "#Form models\n",
    "encoder = Model(inputs=[sketch_features , image_features], outputs=[mu])\n",
    "\n",
    "#Changed Decoder\n",
    "# d_in = Input(shape=[n_z+n_y])\n",
    "\n",
    "input_z = Input(shape=[n_z], name='input_z')\n",
    "d_in = concatenate([input_z, sketch_features])\n",
    "d_h = decoder_hidden(d_in)\n",
    "d_out = decoder_out(d_h)\n",
    "decoder = Model(inputs=[sketch_features, input_z], outputs=[d_out])\n",
    "\n",
    "# Predict the attribute again to enforce its usage\n",
    "attr_int = Dense(internalSize, activation='relu')(reconstr)\n",
    "attr_recons = Dense(n_y, activation='relu', name='recons_output')(attr_int)\n",
    "\n",
    "# Form the VAE model\n",
    "vae = Model(inputs=[sketch_features , image_features], outputs=[reconstr, attr_recons])\n",
    "\n",
    "\n",
    "def vae_loss(y_true, y_pred):\n",
    "    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "    # E[log P(X|z)]\n",
    "    recon = K.mean(K.square(y_pred - y_true), axis=1)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "    kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)\n",
    "    return recon + kl\n",
    "\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "adam = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# sgd = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "vae.compile(optimizer=adam, loss={'decoder_out':vae_loss, 'recons_output':'mean_squared_error'}, loss_weights={'decoder_out':1.0, 'recons_output':10.0})\n",
    "\n",
    "# ============================Load the required data================================== #\n",
    "\n",
    "x = np.array(['1', '2', 'bat', 'cow', 'cabin'])\n",
    "np.save('test_split_ref2', x)\n",
    "# ============================Load the required data================================== #\n",
    "\n",
    "#first load the image features and imagePaths\n",
    "image_VGG_features = np.load('image_vgg_features.npy')            #(12500, 4096)\n",
    "image_paths = np.load('image_paths.npy')                    #(12500, )\n",
    "\n",
    "image_paths = [x for x in image_paths if '.DS_Store' not in x]\n",
    "#next load the sketch_paths\n",
    "sketch_paths = np.load('sketch_paths.npy')                  #(,)\n",
    "sketch_VGG_features = np.load('sketch_vgg_features.npy')    #(,)\n",
    "\n",
    "sketchpaths = [x for x in sketch_paths if '.DS_Store' not in x]\n",
    "\n",
    "#next load the image extension dataset\n",
    "image_VGG_features_ext = np.load('image_ext_vgg_features.npy')    #(73002, 4096)\n",
    "image_paths_ext = np.load('image_ext_paths.npy')            #(73002, )\n",
    "\n",
    "\n",
    "train_sketch_paths = sketch_paths.tolist()\n",
    "#Do a train and test split\n",
    "sketch_paths_per_class = {}\n",
    "for sketchPath in sketch_paths:\n",
    "    className = sketchPath.split('/')[-2]\n",
    "    if className not in sketch_paths_per_class:\n",
    "        sketch_paths_per_class[className] = []\n",
    "    sketch_paths_per_class[className].append(sketchPath)\n",
    "#-------------------------------------------------Non-zero shot split-----------------------------------------------#\n",
    "test_sketch_paths = np.array([])\n",
    "# for className in sketch_paths_per_class:\n",
    "#     sample_paths = np.random.choice(sketch_paths_per_class[className], 50, replace=False)\n",
    "#     test_sketch_paths = np.append(test_sketch_paths, sample_paths)\n",
    "#     for test_path in sample_paths:\n",
    "#         train_sketch_paths.remove(test_path)\n",
    "        \n",
    "# train_sketch_paths = np.array(train_sketch_paths)\n",
    "\n",
    "# print len(train_sketch_paths)\n",
    "# print len(test_sketch_paths)\n",
    "\n",
    "#---------------------------------------------------Zero shot split-------------------------------------------------#\n",
    "\n",
    "test_ref_classes = np.load('test_split_ref2.npy')\n",
    "# 21 classes\n",
    "# array([b'bat', b'cabin', b'cow', b'dolphin', b'door', b'giraffe',\n",
    "#        b'helicopter', b'mouse', b'pear', b'raccoon', b'rhinoceros',\n",
    "#        b'saw', b'scissors', b'seagull', b'skyscraper', b'songbird',\n",
    "#        b'sword', b'tree', b'wheelchair', b'windmill', b'window'],\n",
    "#       dtype='|S10')\n",
    "\n",
    "trainClasses = []\n",
    "for className in sketch_paths_per_class:\n",
    "    if className not in test_ref_classes:\n",
    "        trainClasses.append(className)\n",
    "        continue\n",
    "    else:\n",
    "        test_sketch_paths = np.append(test_sketch_paths, sketch_paths_per_class[className])\n",
    "        test_sketch_paths = [x for x in test_sketch_paths if '.DS_Store' not in x]\n",
    "\n",
    "        for test_path in sketch_paths_per_class[className]:\n",
    "            train_sketch_paths = [x for x in train_sketch_paths if '.DS_Store' not in x]\n",
    "#         train_sketch_paths.remove(test_path)\n",
    "\n",
    "\n",
    "    \n",
    "train_sketch_paths = [x for x in train_sketch_paths if '.DS_Store' not in x]\n",
    "\n",
    "# print len(image_paths)\n",
    "\n",
    "# print (train_sketch_paths) \n",
    "# print (test_sketch_paths) # []\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#form an inverted index for sketch paths\n",
    "sketch_path_index_tracker = {}\n",
    "for idx in range(len(sketch_paths)):\n",
    "    sketch_path_index_tracker[sketch_paths[idx]] = idx\n",
    "\n",
    "#form an inverted index for image paths\n",
    "image_path_index_tracker = {}\n",
    "for idx in range(len(image_paths)):\n",
    "    image_path_index_tracker[image_paths[idx]] = idx\n",
    "\n",
    "\n",
    "\n",
    "# Now seperate features for train and test\n",
    "train_sketch_X = np.zeros((len(train_sketch_paths), n_y))\n",
    "test_sketch_X = np.zeros((len(test_sketch_paths), n_y))\n",
    "\n",
    "for ii in range(len(train_sketch_paths)):\n",
    "    index = sketch_path_index_tracker[train_sketch_paths[ii]]\n",
    "    train_sketch_X[ii,:] = sketch_VGG_features[index, :]\n",
    "\n",
    "for ii in range(len(test_sketch_paths)):\n",
    "    index = sketch_path_index_tracker[test_sketch_paths[ii]]\n",
    "    test_sketch_X[ii,:] = sketch_VGG_features[index, :]\n",
    "\n",
    "def getImagePath(sketchPath):\n",
    "    \"\"\"MY FUNCTION\"\"\"\n",
    "#     temp = sketchPath.replace('sketch_','').split('/')\n",
    "#     for idx in range(len(temp)-1):\n",
    "#         temp =(sketch_paths[idx].replace('sketch_','').split('/')[idx] + '/train/' + sketch_paths[idx].replace('sketch_','').split('/')[idx+1])\n",
    "#         temp= temp.replace('png','jpg')\n",
    "#     return temp \n",
    "    tempArr = sketchPath.replace('sketch_', '').split('-')\n",
    "    imagePath = ''\n",
    "    for idx in range(len(tempArr)): # original (len(x)-1)\n",
    "        imagePath = imagePath + tempArr[idx] + '-'\n",
    "    imagePath = imagePath[:-1]\n",
    "    imagePath = imagePath.replace('png', 'jpg') # ORIGINAL\n",
    "    return imagePath\n",
    "\n",
    "\n",
    "# Combine parallel images and sketches\n",
    "train_X_img = np.zeros((len(train_sketch_paths), n_x))\n",
    "for idx in range(len(train_sketch_paths)):\n",
    "    imagePath = getImagePath(train_sketch_paths[idx])\n",
    "    \n",
    "    imageIdx = image_path_index_tracker[imagePath]\n",
    "    train_X_img[idx,:] = image_VGG_features[imageIdx]\n",
    "    \n",
    "\n",
    "test_X_img = np.zeros((len(test_sketch_paths), n_x ))\n",
    "for idx in range(len(test_sketch_paths)):\n",
    "    imagePath = getImagePath(test_sketch_paths[idx]) \n",
    "    imageIdx = image_path_index_tracker[imagePath]\n",
    "    test_X_img[idx,:] = image_VGG_features[imageIdx]\n",
    "\n",
    "\n",
    "# =========================== TEST RETREIVAL ======================================#\n",
    "\n",
    "#Build a nearest neighbour classifier\n",
    "NEIGH_NUM = 200\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# , LSHForest\n",
    "# combined_img_features = np.concatenate((image_VGG_features, image_VGG_features_ext))\n",
    "\n",
    "#-----------------------------------Generalized zero shot setting-------------------------------#\n",
    "\n",
    "# nbrs = NearestNeighbors(n_neighbors=NEIGH_NUM, metric='cosine', algorithm='brute').fit(image_VGG_features_ext)\n",
    "\n",
    "#--------------------------------Non-generalized zero shot setting-------------------------------#\n",
    "\n",
    "#remove all the training class images\n",
    "image_paths_ext_index_tracker = {}\n",
    "for idx in range(len(image_paths_ext)):\n",
    "    image_paths_ext_index_tracker[image_paths_ext[idx]] = idx\n",
    "\n",
    "con_image_paths_ext = []\n",
    "for path in image_paths_ext:\n",
    "    className = path.split(b'/')[-2]\n",
    "    if className not in trainClasses:\n",
    "        con_image_paths_ext.append(path)\n",
    "\n",
    "con_img_VGG_features_ext = np.zeros((len(con_image_paths_ext), 4096))\n",
    "for idx in range(len(con_image_paths_ext)):\n",
    "    originalIndex = image_paths_ext_index_tracker[con_image_paths_ext[idx]]\n",
    "    con_img_VGG_features_ext[idx, :] = image_VGG_features_ext[originalIndex, :] \n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=NEIGH_NUM, metric='cosine', algorithm='brute').fit(con_img_VGG_features_ext)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#testing on test queries\n",
    "\n",
    "# image_classes = np.array([path.split('/')[-2] for path in image_paths_ext])\n",
    "image_classes = np.array([path.split(b'/')[-2] for path in con_image_paths_ext])\n",
    "\n",
    "test_sketch_classes = np.array([path.split('/')[-2] for path in test_sketch_paths])\n",
    "\n",
    "\n",
    "def mapChange(inputArr):\n",
    "    dup = np.copy(inputArr)\n",
    "    for idx in range(inputArr.shape[1]):\n",
    "        if (idx != 0):\n",
    "            dup[:,idx] = dup[:,idx-1] + dup[:,idx]\n",
    "    return np.multiply(dup, inputArr)\n",
    "\n",
    "#Use multiple z while prediction\n",
    "#Write a function for prediction of precision\n",
    "#Uses average of all predicted features for retrieval\n",
    "def find_precision():\n",
    "    RANDOM_Z_PER_SKETCH = 100\n",
    "    noiseIP = np.random.normal(size=[RANDOM_Z_PER_SKETCH*len(test_sketch_paths) , n_z])\n",
    "    sketchIP = np.zeros([RANDOM_Z_PER_SKETCH*len(test_sketch_paths) , n_y])\n",
    "    for ii in range(0,len(test_sketch_paths)):\n",
    "        for jj in range(0 , RANDOM_Z_PER_SKETCH):\n",
    "            sketchIP[ii*RANDOM_Z_PER_SKETCH + jj] = test_sketch_X[ii]\n",
    "    print('Predicting...')\n",
    "    predImageFeatures = decoder.predict({'sketch_features' : sketchIP , 'input_z' : noiseIP} , verbose=1)\n",
    "    avgPredImgFeatures = np.zeros([len(test_sketch_paths) , n_x])\n",
    "    for ii in range(0 , len(test_sketch_paths)):\n",
    "        avgPredImgFeatures[ii] = np.mean(predImageFeatures[ii*RANDOM_Z_PER_SKETCH:(ii+1)*RANDOM_Z_PER_SKETCH] , axis=0)\n",
    "    \n",
    "    # From here...\n",
    "    distances, indices = nbrs.kneighbors(avgPredImgFeatures)\n",
    "    retrieved_classes = image_classes[indices]\n",
    "    results = np.zeros(retrieved_classes.shape)\n",
    "    for idx in range(results.shape[0]):\n",
    "        results[idx] = (retrieved_classes[idx] == test_sketch_classes[idx])\n",
    "    precision_200 = np.mean(results, axis=1)\n",
    "    temp = [np.arange(200) for ii in range(results.shape[0])]\n",
    "    mAP_term = 1.0/(np.stack(temp, axis=0) + 1)\n",
    "    mAP = np.mean(np.multiply(mapChange(results), mAP_term), axis=1)\n",
    "    print ('')\n",
    "    print ('The mean precision@200 for test sketches is ' + str(np.mean(precision_200)))\n",
    "    print ('The mAP for test_sketches is ' + str(np.mean(mAP)))\n",
    "    return np.mean(precision_200)\n",
    "\n",
    "#===============================Training the model============================================#\n",
    "\n",
    "\n",
    "prec = 0\n",
    "\n",
    "vae.fit({'sketch_features': train_sketch_X , 'image_features': train_X_img }, [train_X_img, train_sketch_X] , batch_size=BATCH_SIZE, nb_epoch=MAX_EPOCH)\n",
    "find_precision()\n",
    "\n",
    "# Use min as metric to retrieve closest\n",
    "# find_precision_min()\n",
    "\n",
    "\n",
    "def find_precision_kmeans():\n",
    "    RANDOM_Z_PER_SKETCH = 100\n",
    "    noiseIP = np.random.normal(size=[RANDOM_Z_PER_SKETCH*len(test_sketch_paths) , n_z])\n",
    "    sketchIP = np.zeros([RANDOM_Z_PER_SKETCH*len(test_sketch_paths) , n_y])\n",
    "    for ii in range(0,len(test_sketch_paths)):\n",
    "        for jj in range(0 , RANDOM_Z_PER_SKETCH):\n",
    "            sketchIP[ii*RANDOM_Z_PER_SKETCH + jj] = test_sketch_X[ii]\n",
    "    print ('Predicting...')\n",
    "    predImageFeatures = decoder.predict({'sketch_features' : sketchIP , 'input_z' : noiseIP} , verbose=1)\n",
    "        \n",
    "\n",
    "#use min(d,{Z}) as the distance measure\n",
    "def find_precision_min():\n",
    "    RANDOM_Z_PER_SKETCH = 10\n",
    "    noiseIP = np.random.normal(size=[RANDOM_Z_PER_SKETCH*len(test_sketch_paths) , n_z])\n",
    "    sketchIP = np.zeros([RANDOM_Z_PER_SKETCH*len(test_sketch_paths) , n_y])\n",
    "    for ii in range(0,len(test_sketch_paths)):\n",
    "        for jj in range(0 , RANDOM_Z_PER_SKETCH):\n",
    "            sketchIP[ii*RANDOM_Z_PER_SKETCH + jj] = test_sketch_X[ii]\n",
    "    print ('Predicting...')\n",
    "    predImageFeatures = decoder.predict({'sketch_features' : sketchIP , 'input_z' : noiseIP} , verbose=1)\n",
    "    #find the closest for each prediction\n",
    "    distances = np.zeros((len(test_sketch_paths)*RANDOM_Z_PER_SKETCH, NEIGH_NUM))\n",
    "    indices = np.zeros((len(test_sketch_paths)*RANDOM_Z_PER_SKETCH, NEIGH_NUM))\n",
    "    PRED_BATCH_SIZE = 625\n",
    "    for ii in range(PRED_BATCH_SIZE):\n",
    "        distances[ii*PRED_BATCH_SIZE:(ii+1)*PRED_BATCH_SIZE], indices[ii*PRED_BATCH_SIZE:(ii+1)*PRED_BATCH_SIZE] = nbrs.kneighbors(predImageFeatures[ii*PRED_BATCH_SIZE:(ii+1)*PRED_BATCH_SIZE])\n",
    "    comb_distances = np.zeros((len(test_sketch_paths), RANDOM_Z_PER_SKETCH*NEIGH_NUM))\n",
    "    comb_indices = np.zeros((len(test_sketch_paths), RANDOM_Z_PER_SKETCH*NEIGH_NUM))\n",
    "    for ii in range(len(test_sketch_paths)):\n",
    "        for jj in range(RANDOM_Z_PER_SKETCH):\n",
    "            comb_distances[ii,jj*NEIGH_NUM:(jj+1)*NEIGH_NUM] = distances[ii*RANDOM_Z_PER_SKETCH + jj, :]\n",
    "            comb_indices[ii,jj*NEIGH_NUM:(jj+1)*NEIGH_NUM] = indices[ii*RANDOM_Z_PER_SKETCH + jj, :]            \n",
    "    #next reduce these indices to top 200\n",
    "    #first sort the array\n",
    "    for ii in range(len(test_sketch_paths)):\n",
    "        arrIdx = comb_distances[ii].argsort()\n",
    "        comb_distances[ii] = comb_distances[ii][arrIdx]\n",
    "        comb_indices[ii] = comb_indices[ii][arrIdx]\n",
    "    #then get top 200 without dupliactes\n",
    "    top_indices = np.zeros((len(test_sketch_paths), NEIGH_NUM)).astype(int)\n",
    "    for ii in range(len(test_sketch_paths)):\n",
    "        top_indices[ii,:] = pd.unique(comb_indices[ii,:])[:NEIGH_NUM]\n",
    "    retrieved_classes = image_classes[top_indices]\n",
    "    results = np.zeros(retrieved_classes.shape)\n",
    "    for idx in range(results.shape[0]):\n",
    "        results[idx] = (retrieved_classes[idx] == test_sketch_classes[idx])\n",
    "    precision_200 = np.mean(results, axis=1)\n",
    "    temp = [np.arange(200) for ii in range(results.shape[0])]\n",
    "    mAP_term = 1.0/(np.stack(temp, axis=0) + 1)\n",
    "    mAP = np.mean(np.multiply(mapChange(results), mAP_term), axis=1)\n",
    "    print ('')\n",
    "    print ('The mean precision@200 using min metric for test sketches is ' + str(np.mean(precision_200)))\n",
    "    print ('The mAP for test_sketches using min metric is ' + str(np.mean(mAP)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prestep_sketch\n",
    "#Load VGG-net and save the image features in a dictionary\n",
    "# source /media/data/ashish/tensorflow/tensorflow//bin/activate\n",
    "import numpy as np\n",
    "import keras\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import backend as K\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "def get_session(gpu_fraction=0.5):\n",
    "    '''Assume that you have 6GB of GPU memory and want to allocate ~2GB'''\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# KTF.set_session(get_session())\n",
    "tf.keras.backend.set_session(\n",
    "    get_session()\n",
    ")\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "\n",
    "\n",
    "\n",
    "#Load VGG-16 as image model due to lack of proper image data\n",
    "vgg_model = vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "vgg_model.layers.pop()\n",
    "vgg_model.layers[-1].outbound_nodes = []\n",
    "vgg_model.outputs = [vgg_model.layers[-1].output]\n",
    "vgg_model.summary()\n",
    "\n",
    "\n",
    "#Store all sketch file names\n",
    "\n",
    "#------------------------Change Sketchy Path here---------------------------------#\n",
    "sketch_root = \"sketch_images\"\n",
    "#---------------------------------------------------------------------------------#\n",
    "#Store all the sketch paths\n",
    "sketch_paths = []\n",
    "for path, subdirs, files in os.walk(sketch_root):\n",
    "    for fileName in files:\n",
    "        sketch_paths.append(path + '/' + fileName)\n",
    "\n",
    "\n",
    "np.save('sketch_paths', np.array(sketch_paths))\n",
    "\n",
    "\n",
    "# for sketches\n",
    "sketch_paths = np.load('sketch_paths.npy')\n",
    "\n",
    "BATCH_SIZE = 41\n",
    "X_out = np.zeros((len(sketch_paths), 4096))\n",
    "X_in = np.zeros((BATCH_SIZE, 224, 224, 3))\n",
    "for ii in range(len(sketch_paths)//BATCH_SIZE): ## FLAT\n",
    "    print ('Batch ' + str(ii) + ' in progress...')\n",
    "    for jj in range(BATCH_SIZE):\n",
    "        X_in[jj,:,:,:] = image.img_to_array( image.load_img(sketch_paths[ii*BATCH_SIZE + jj], target_size=(224, 224)) )\n",
    "    X_in = preprocess_input(X_in)\n",
    "    X_out[ii*BATCH_SIZE:(ii+1)*BATCH_SIZE, :] = vgg_model.predict_on_batch(X_in)\n",
    "\n",
    "\n",
    "#store the image paths and vgg_features\n",
    "np.save('vgg_sketch_features_mod', X_out)\n",
    "np.save('sketch_paths', np.array(sketch_paths))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Batch 0 in progress...\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file 'images/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-6e77105b5be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Batch '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' in progress...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mX_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mX_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mX_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2929\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2930\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 2931\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2932\u001b[0m     )\n\u001b[1;32m   2933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file 'images/.DS_Store'"
     ]
    }
   ],
   "source": [
    "# prestep_image \n",
    "#Load VGG-net and save the image features in a dictionary\n",
    "# source /media/data/ashish/tensorflow/tensorflow//bin/activate\n",
    "import numpy as np\n",
    "import keras\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import backend as K\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "def get_session(gpu_fraction=0.5):\n",
    "    '''Assume that you have 6GB of GPU memory and want to allocate ~2GB'''\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# KTF.set_session(get_session())\n",
    "tf.keras.backend.set_session(get_session())\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "\n",
    "\n",
    "\n",
    "#Load VGG-16 as image model due to lack of proper image data\n",
    "vgg_model = vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "vgg_model.layers.pop()\n",
    "vgg_model.layers[-1].outbound_nodes = []\n",
    "vgg_model.outputs = [vgg_model.layers[-1].output]\n",
    "vgg_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "#Store all image file names\n",
    "#-------------------------------------Change Sketchy image path here----------------------------#\n",
    "image_root = \"images\"\n",
    "#-----------------------------------------------------------------------------------------------#\n",
    "#Store all the image paths\n",
    "image_paths = []\n",
    "for path, subdirs, files in os.walk(image_root):\n",
    "    for fileName in files:\n",
    "        image_paths.append(path + '/' + fileName)\n",
    "\n",
    "\n",
    "np.save('image_paths', np.array(image_paths))\n",
    "\n",
    "\n",
    "\n",
    "#Generate vgg features for each image \n",
    "BATCH_SIZE = 25\n",
    "X_out = np.zeros((len(image_paths), 4096))\n",
    "X_in = np.zeros((BATCH_SIZE, 224, 224, 3))\n",
    "for ii in range(len(image_paths)//BATCH_SIZE): # ORIGINALLY / not //\n",
    "    print ('Batch ' + str(ii) + ' in progress...')\n",
    "    for jj in range(BATCH_SIZE):\n",
    "        X_in[jj,:,:,:] = image.img_to_array( image.load_img(image_paths[ii*BATCH_SIZE + jj], target_size=(224, 224)) )\n",
    "    X_in = preprocess_input(X_in)\n",
    "    X_out[ii*BATCH_SIZE:(ii+1)*BATCH_SIZE, :] = vgg_model.predict_on_batch(X_in)\n",
    "\n",
    "#store the image paths and vgg_features\n",
    "np.save('vgg_features_mod', X_out)\n",
    "np.save('image_paths', np.array(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Batch 0/2.5 in progress...\n",
      "Batch 1/2.5 in progress...\n"
     ]
    }
   ],
   "source": [
    "# EXT\n",
    "#Load VGG-net and save the image features in a dictionary\n",
    "import numpy as np\n",
    "import keras\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import backend as K\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "def get_session(gpu_fraction=1.0):\n",
    "    '''Assume that you have 6GB of GPU memory and want to allocate ~2GB'''\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# KTF.set_session(get_session())\n",
    "tf.keras.backend.set_session(get_session())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n",
    "\n",
    "\n",
    "\n",
    "#Load VGG-16 as image model due to lack of proper image data\n",
    "vgg_model = vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "vgg_model.layers.pop()\n",
    "vgg_model.layers[-1].outbound_nodes = []\n",
    "vgg_model.outputs = [vgg_model.layers[-1].output]\n",
    "vgg_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "#----------------------------------------Change extension path here-------------------------#\n",
    "image_root = \"images/train\"\n",
    "#-------------------------------------------------------------------------------------------#\n",
    "#Store all the image paths\n",
    "image_paths = []\n",
    "# image_labels = np.zeros(12500, ) -> not needed currently\n",
    "for path, subdirs, files in os.walk(image_root):\n",
    "    for fileName in files:\n",
    "        image_paths.append(path + '/' + fileName)\n",
    "\n",
    "#Generate vgg features for each image \n",
    "BATCH_SIZE = 46\n",
    "X_out = np.zeros((len(image_paths), 4096))\n",
    "X_in = np.zeros((BATCH_SIZE, 224, 224, 3))\n",
    "for ii in range(len(image_paths)//BATCH_SIZE): # not // orginally /\n",
    "    print ('Batch ' + str(ii) + '/' + str(len(image_paths)/BATCH_SIZE) + ' in progress...')\n",
    "    for jj in range(BATCH_SIZE):\n",
    "        X_in[jj,:,:,:] = image.img_to_array( image.load_img(image_paths[ii*BATCH_SIZE + jj], target_size=(224, 224)) )\n",
    "    X_in = preprocess_input(X_in)\n",
    "    X_out[ii*BATCH_SIZE:(ii+1)*BATCH_SIZE, :] = vgg_model.predict_on_batch(X_in)\n",
    "\n",
    "#store the image paths and vgg_features\n",
    "np.save('vgg_features_ext_mod', X_out)\n",
    "np.save('image_paths_ext', np.array(image_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
