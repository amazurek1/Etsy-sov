{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "src = cv2.imread('images/lamp.jpeg', 1)\n",
    "blurred = cv2.GaussianBlur(src, (5, 5), 0)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = tf.keras.Sequential([\n",
    "#     hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_224/classification/4\")\n",
    "# ])\n",
    "# m.build([None, 224, 224, 3])  # Batch input shape.\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "#    tf.compat.v1.saved_model.loader.load(\n",
    "#        sess,\n",
    "#        [tf.compat.v1.saved_model.tag_constants.SERVING],\n",
    "#        'mobile/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-381ccc886798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblurred_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblurred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# edgeDetector = cv2.ximgproc.createStructuredEdgeDetection('model.yml')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# edges = edgeDetector.detectEdges(blurred_float) * 255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# cv2.imwrite('edge-raw.jpg', edges)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "blurred_float = blurred.astype(np.float32) / 255.0\n",
    "edgeDetector = cv2.ximgproc.createStructuredEdgeDetection('model.yml')\n",
    "edges = edgeDetector.detectEdges(blurred_float) * 255.0\n",
    "cv2.imwrite('edge-raw.jpg', edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filterOutSaltPepperNoise(edgeImg):\n",
    "    # Get rid of salt & pepper noise.\n",
    "    count = 0\n",
    "    lastMedian = edgeImg\n",
    "    median = cv2.medianBlur(edgeImg, 3)\n",
    "    while not np.array_equal(lastMedian, median):\n",
    "        # get those pixels that gets zeroed out\n",
    "        zeroed = np.invert(np.logical_and(median, edgeImg))\n",
    "        edgeImg[zeroed] = 0\n",
    "\n",
    "        count = count + 1\n",
    "        if count > 70:\n",
    "            break\n",
    "        lastMedian = median\n",
    "        median = cv2.medianBlur(edgeImg, 3)\n",
    "\n",
    "\n",
    "edges_8u = np.asarray(edges, np.uint8)\n",
    "filterOutSaltPepperNoise(edges_8u)\n",
    "cv2.imwrite('edge.jpg', edges_8u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findSignificantContour(edgeImg):\n",
    "    contours, hierarchy = cv2.findContours(\n",
    "        edgeImg,\n",
    "        cv2.RETR_TREE,\n",
    "        cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "       # Find level 1 contours\n",
    "    level1Meta = []\n",
    "    for contourIndex, tupl in enumerate(hierarchy[0]):\n",
    "        # Each array is in format (Next, Prev, First child, Parent)\n",
    "        # Filter the ones without parent\n",
    "        if tupl[3] == -1:\n",
    "            tupl = np.insert(tupl.copy(), 0, [contourIndex])\n",
    "            level1Meta.append(tupl)\n",
    "\n",
    "    # From among them, find the contours with large surface area.\n",
    "    contoursWithArea = []\n",
    "    for tupl in level1Meta:\n",
    "        contourIndex = tupl[0]\n",
    "        contour = contours[contourIndex]\n",
    "        area = cv2.contourArea(contour)\n",
    "        contoursWithArea.append([contour, area, contourIndex])\n",
    "\n",
    "    contoursWithArea.sort(key=lambda meta: meta[1], reverse=True)\n",
    "    largestContour = contoursWithArea[0][0]\n",
    "    return largestContour\n",
    "\n",
    "\n",
    "contour = findSignificantContour(edges_8u)\n",
    "# Draw the contour on the original image\n",
    "contourImg = np.copy(src)\n",
    "cv2.drawContours(contourImg, [contour], 0, (0, 255, 0), 2, cv2.LINE_AA, maxLevel=1)\n",
    "cv2.imwrite('contour.jpg', contourImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.zeros_like(edges_8u)\n",
    "cv2.fillPoly(mask, [contour], 255)\n",
    "\n",
    "# calculate sure foreground area by dilating the mask\n",
    "mapFg = cv2.erode(mask, np.ones((5, 5), np.uint8), iterations=10)\n",
    "\n",
    "# mark inital mask as \"probably background\"\n",
    "# and mapFg as sure foreground\n",
    "trimap = np.copy(mask)\n",
    "trimap[mask == 0] = cv2.GC_BGD\n",
    "trimap[mask == 255] = cv2.GC_PR_BGD\n",
    "trimap[mapFg == 255] = cv2.GC_FGD\n",
    "\n",
    "# visualize trimap\n",
    "trimap_print = np.copy(trimap)\n",
    "trimap_print[trimap_print == cv2.GC_PR_BGD] = 128\n",
    "trimap_print[trimap_print == cv2.GC_FGD] = 255\n",
    "cv2.imwrite('trimap.png', trimap_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run grabcut\n",
    "bgdModel = np.zeros((1, 65), np.float64)\n",
    "fgdModel = np.zeros((1, 65), np.float64)\n",
    "rect = (0, 0, mask.shape[0] - 1, mask.shape[1] - 1)\n",
    "cv2.grabCut(src, trimap, rect, bgdModel, fgdModel, 5, cv2.GC_EVAL) # cv2.GC_INIT_WITH_MASK\n",
    "\n",
    "# create mask again\n",
    "mask2 = np.where(\n",
    "    (trimap == cv2.GC_FGD) | (trimap == cv2.GC_PR_FGD),\n",
    "    255,\n",
    "    0\n",
    ").astype('uint8')\n",
    "cv2.imwrite('mask2.jpg', mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contour2 = findSignificantContour(mask2) \n",
    "mask3 = np.zeros_like(mask2)\n",
    "cv2.fillPoly(mask3, [contour2], 255)  ### ORIGINAL CONTOUR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blended alpha cut-out\n",
    "mask3 = np.repeat(mask3[:, :, np.newaxis], 3, axis=2)\n",
    "mask4 = cv2.GaussianBlur(mask3, (3, 3), 0)\n",
    "alpha = mask4.astype(float) * 1.1  # making blend stronger\n",
    "alpha[mask3 > 0] = 255\n",
    "alpha[alpha > 255] = 255\n",
    "alpha = alpha.astype(float)\n",
    "\n",
    "foreground = np.copy(src).astype(float)\n",
    "foreground[mask4 == 0] = 0\n",
    "background = np.ones_like(foreground, dtype=float) * 255\n",
    "\n",
    "cv2.imwrite('foreground.png', foreground)\n",
    "cv2.imwrite('background.png', background)\n",
    "cv2.imwrite('alpha.png', alpha)\n",
    "\n",
    "# Normalize the alpha mask to keep intensity between 0 and 1\n",
    "alpha = alpha / 255.0\n",
    "# Multiply the foreground with the alpha matte\n",
    "foreground = cv2.multiply(alpha, foreground)\n",
    "# Multiply the background with ( 1 - alpha )\n",
    "background = cv2.multiply(1.0 - alpha, background)\n",
    "# Add the masked foreground and background.\n",
    "cutout = cv2.add(foreground, background)\n",
    "\n",
    "cv2.imwrite(f'test1/cutout.jpg', cutout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
